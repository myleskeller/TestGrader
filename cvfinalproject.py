# -*- coding: utf-8 -*-
"""CVFinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C_xag0p4MkLfrPOihxTeESGrS2IhYgsa
"""

#@title Global Parameters
figure_size = "Large" #@param ["Large", "Medium", "Small"]

if figure_size == "Large":
  figure_dims = (40,20)
if figure_size == "Medium":
  figure_dims = (20,10)
if figure_size == "Small":
  figure_dims = (10,5)

"""# initialization"""

#@title google drive
enabled = True #@param {type:"boolean"}
if (enabled):
  from google.colab import drive
  drive.mount('/content/drive')
  !ls /content/drive/MyDrive/image_data

#@title python imports / pip installs { vertical-output: true }
import numpy as np
import scipy.ndimage as scimage
import matplotlib.pyplot as plt
import torch
import re

from skimage import io
from skimage.transform import warp
import imutils
from imutils.perspective import four_point_transform
from imutils import contours

!sudo apt install tesseract-ocr
!pip install pytesseract
import pytesseract
from pytesseract import Output

from PIL import Image
from PIL import ImageStat

!pip install opencv-contrib-python==4.4.0.44
import cv2
print (cv2 .__version__)

#@title import test images
enabled = False #@param {type:"boolean"}
if (enabled):
  # answer key
  key_ideal = io.imread('https://github.com/myleskeller/testGrader/raw/main/gt_ideal.jpg')
  key_angled = io.imread('https://github.com/myleskeller/testGrader/raw/main/gt_angle.jpg')
  key_blurry = io.imread('https://github.com/myleskeller/testGrader/raw/main/gt_blurry.jpg')

  # user completed
  ideal = io.imread('https://github.com/myleskeller/testGrader/raw/main/ideal.jpg')
  angled = io.imread('https://github.com/myleskeller/testGrader/raw/main/angled.jpg')
  blurry = io.imread('https://github.com/myleskeller/testGrader/raw/main/blurry.jpg')

"""# functions"""

#Uses canny edge detection and four point transform to isolate the sheet paper from the rest of the image
def getPageOutline(image):
  ds_ratio = len(image[0]) / 500.0 #downscaling the image helped with our edge detection
  downscaled = cv2.resize(image, (0,0), fx=1/ds_ratio, fy=1/ds_ratio)

  gray = cv2.cvtColor(downscaled, cv2.COLOR_BGR2GRAY) #cvt to grayscale
  gray = cv2.bilateralFilter(gray, 11, 17, 17)  #edge preserving but smooths image
 
  edged = cv2.Canny(gray, 250, 300) #canny edge detection
  edged = cv2.resize(edged, (0,0), fx=ds_ratio, fy=ds_ratio)#resizing image so we get full sized output.

  contours = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)#find contours
  contours = imutils.grab_contours(contours)
  testContours = None
  if len(contours) > 0: # at least one contour should be found
    contours = sorted(contours, key=cv2.contourArea, reverse=True) # sort the contours to start searching from largest

    for x in contours:
      perimeter = cv2.arcLength(x, True) #approximate length of the contour
      approximation = cv2.approxPolyDP(x, 0.02 * perimeter, True)
      if len(approximation) == 4: #stop after papers four edges found.
        testContours = approximation
        return x

#From OpenCV samples
#https://github.com/opencv/opencv/blob/master/samples/python/squares.py#L21
def angle_cos(p0, p1, p2):
  d1, d2 = (p0-p1).astype('float'), (p2-p1).astype('float')
  return abs( np.dot(d1, d2) / np.sqrt( np.dot(d1, d1)*np.dot(d2, d2) ) )

def find_rectangles(img):
  threshold = min([img.shape[0], img.shape[1]]) * 0.4

  img = cv2.GaussianBlur(img, (5, 5), 0)
  squares = []

  for gray in cv2.split(img):
      for thrs in range(0, 255, 26):
          if thrs == 0:
              bin = cv2.Canny(gray, 0, 50, apertureSize=5)
              # plt.figure(figsize=(40,20))
              # plt.imshow(bin)
              # plt.title("Canny")

              bin = cv2.dilate(bin, None)
              # plt.figure(figsize=(40,20))
              # plt.imshow(bin)
              # plt.title("Dilate")
          else:
              _retval, bin = cv2.threshold(gray, thrs, 255, cv2.THRESH_BINARY)
          #contours = cv2.findContours(bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
          contours = cv2.findContours(bin, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
          contours = contours[0] if len(contours) == 2 else contours[1]

          for cnt in contours:
              cnt_len = cv2.arcLength(cnt, True)
              cnt = cv2.approxPolyDP(cnt, 0.02*cnt_len, True)

              if len(cnt) == 4 and cv2.contourArea(cnt) > threshold and cv2.isContourConvex(cnt):
                  cnt = cnt.reshape(-1, 2)
                  max_cos = np.max([angle_cos( cnt[i], cnt[(i+1) % 4], cnt[(i+2) % 4] ) for i in range(4)])
                  #print(cnt)
                  a = (cnt[1][1] - cnt[0][1])

                  if max_cos < 0.1 and a < img.shape[0]*0.8:
                      squares.append(cnt)

  # print (len(squares))
  return squares

def getCircles(img, percentage, debug = False):
  cimg = img.copy() # numpy function

  # Convert to grayscale.
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
  # Blur using 3 * 3 kernel.
  gray_blurred = cv2.blur(gray, (3, 3))
    
  # Apply Hough transform on the blurred image.
  circles = cv2.HoughCircles(gray_blurred, 
                    cv2.HOUGH_GRADIENT, 1, 20, param1 = 50, param2 = 30, 
                minRadius =int( img.shape[1]*0.02254902 * (1-percentage)), 
                    maxRadius = int(img.shape[1]*0.02254902 * (1+percentage)) )
  
  if circles is not None: # Check if circles have been found and only then iterate over these and add them to the image
    circles = np.uint16(np.around(circles))
    _a, b, _c = circles.shape

    for i in range(b):
        cv2.circle(cimg, (circles[0][i][0], circles[0][i][1]), circles[0][i][2], (0, 0, 255), 3, cv2.LINE_AA)
        cv2.circle(cimg, (circles[0][i][0], circles[0][i][1]), 2, (0, 255, 0), 3, cv2.LINE_AA)  # draw center of circle
    if(debug):
      plt.figure(figsize=(figure_dims))
      plt.imshow(cimg)
      plt.title("Detected Circles")
      plt.axis('off')
      plt.show()

  #plt.imshow(cv2.imshow("source", src))

  #plt.imshow(cv2.drawContours(testkey.copy(), rectangles, -1, (0, 255, 0), 3))

  # print('Done')
  return circles

def groupCircles(circles):# this function groups the circles in a n(number of questions) by m(number of possible answer) array
  grouped_circles = [];
  # sort circles by y coordinate
  sorted_circles = sorted(circles[0] , key=lambda item: [item[1]])
  i = 0
  
  while i < len(sorted_circles):
    j = i +1
    current_row = [] # add all circles from current row(same y coordinates) to array

    while j < len(sorted_circles):
      if ( abs (int(sorted_circles[i][1]) - int(sorted_circles[j][1])) < int(sorted_circles[i][2])): # if y coordinates between circles are close, within the radius of the circle, add to current array
        current_row.append(sorted_circles[j])
        sorted_circles = sorted_circles[ : j]  + sorted_circles[j+1 : ] #removes circle at index j        
        j=j-1
      j=j+1
    current_row.append(sorted_circles[i])#after checking all other circles, add current circle
    grouped_circles.append(current_row) # add array of current row of circles to array of total questions
    i=i+1
    
  for i in range(len(grouped_circles)):
    grouped_circles[i] = sorted(grouped_circles[i] , key=lambda item: [item[0]])

  return grouped_circles;

def getDiagonal(rectangle):# this function returns the bottomleftmost and toprightmost vertices
  sorted_vertices = sorted(rectangle , key=lambda item: [item[0], item[1]])# sort vertices by x coordinate then y coordinate
  minimum = sorted_vertices[0] #minimum is first vertices
  maximum = sorted_vertices[3] #maximum is last vertices
  if(minimum[1] > sorted_vertices[1][1]):  #if the y coord of minimum is greater than the second entry in ordered vertices, than set minimum to the second entry(want bottom left, not top left)
    minimum =  sorted_vertices[1]
  if(maximum[1] < sorted_vertices[2][1]): #if the y coord of maximum is less than the second to lastentry in ordered vertices, than set maximum to the second to last entry(want top right, not botoom right)
    maximum = sorted_vertices[2]
  return np.array([minimum, maximum])
  
def removeDuplicates(image_width, rectangles): # this function gets rid of redundent bounded boxes by comparing the location of their their diagonal vertices
  threshold = image_width * 0.04; #tolerance of ~4% of page width
  i = 0
  # print(type(rectangles))

  while (i <len(rectangles)):
    j = i+1
    while(j<len(rectangles)):
      if(abs(getDiagonal(rectangles[i]) - getDiagonal(rectangles[j])) < threshold).all(): # if the diagnal vertices of the rectangles are within a thresholded range of eachother
        rectangles = rectangles[ : j]  + rectangles[j+1 : ] #remove entry j from list
        j=j-1
      j=j+1
    i=i+1

  return rectangles

def affineTransformPage(outline, img):# this function crops the test using the largest bounding box and does four point transform
  perimeter = cv2.arcLength(outline, True) #approximate length of the contour
  approximation = cv2.approxPolyDP(outline, 0.02 * perimeter, True)
  testContours = approximation
  img = four_point_transform(img, testContours.reshape(4, 2))

  # if image is landscape, make it portrait (may sill be upside down)
  if (img.shape[1]> img.shape[0]):
    img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)

  return img

def findTopGlyphs(glyphs, page_height):
  glyph_key = []
  for i in range(len(glyphs)):
    j = i +1
    while(j<len(glyphs)):
      #print((glyphs[i] - glyphs[j])[:,0])
      if(abs((glyphs[i] - glyphs[j])[:,0]) < 2*page_height*0.022727273).all(): # make sure both squares are roughly at same y coordinates (DO RATIO)
        glyph_key.append(glyphs[i])
        glyph_key.append(glyphs[j])
        return glyph_key
      j = j+1
 
def correctPageOrientation(img, debug = False):# this function takes the cropped image and ensures it is correctly oriented
  rectangles = find_rectangles(img)
  if(debug):
    # print("glyphs:", rectangles) 
    plt.figure(figsize=(figure_dims))
    plt.imshow(cv2.drawContours(img.copy(), rectangles, -1, (0, 255, 0), 3))
    plt.title("Detected Rectangles")
    plt.axis('off')
    plt.show()
  rectangles = removeDuplicates(min([img.shape[0], img.shape[1]]), rectangles)
  glyphs = []

  for rec in rectangles: #this function finds all of the glyphs or fiducial markers(3 squares in image corners)
    diag = getDiagonal(rec)
    x_disp = (diag[1, 1] - diag[0,1]) 
    y_disp = (diag[1,0] - diag[0,0]) 
    if(x_disp/y_disp > .8 and abs(x_disp/img.shape[1] - 0.029411765) < .2): #find gliphs by making sure width and height are roughly the same, and width holds a ratio with the template.
      glyphs.append(rec)
  if(debug):
    # print("glyphs:", glyphs) 
    plt.figure(figsize=(figure_dims))
    plt.imshow(cv2.drawContours(img.copy(), glyphs, -1, (0, 255, 0), 3))
    plt.title("Detected Fiducials")
    plt.axis('off')
    plt.show()

  glyph_key = findTopGlyphs(glyphs, img.shape[0]) #find two glyphs that are at same y coordinates
  
  # if(debug):
  #   print("glyph_key",glyph_key)

  if(glyph_key[0][0][0] > img.shape[0]/2): # if glyph y coordinate is lower than the center of the page
  # if(glyph_key[0][1] > img.shape[0]/2).all(): # if glyph y coordinate is lower than the center of the page
    img = cv2.rotate(img, cv2.ROTATE_180)

  return img

def index2ABCD(index):
  # convert index integer to alphabet character, where 0 = 'A'
  return chr(index + 65)

def avgLuminance(roi):
  #find average luminance of image data inside circle
  roi = Image.fromarray(roi, 'RGB') #ImageStat is so extra...
  stat = ImageStat.Stat(roi)
  return stat.mean[0]

def determineAnswer(img, grouped_circles, debug = False):
  answers = []

  # for each question's worth of circles..
  for i in range(len(grouped_circles)):
    circle_luminance = []
    for j in range(len(grouped_circles[i])):

      # get each circles parameters
      x = grouped_circles[i][j][0]
      y = grouped_circles[i][j][1]
      r = grouped_circles[i][j][2]

      # create a region of intterest containing all of the pixel data in the circle
      circle = img[y-r:y+r, x-r:x+r]

      # annotate the average luminance of all pixels within the circle
      circle_luminance.append(avgLuminance(circle))

      # choose the circle with the "darkest" luminance as the picked answer
      answer = index2ABCD(circle_luminance.index(min(circle_luminance)))

      if(debug):
        plt.subplot(1, len(grouped_circles[i]), j+1)
        plt.axis('off')
        subtitle = str( str(index2ABCD(len(circle_luminance)-1)) + ": " + str(round(avgLuminance(circle), 0)) )
        # subtitle = "Question #" + str(i+1) + str(index2ABCD(len(circle_luminance)-1) + ", L:" + str(round(avgLuminance(circle), 1)))
        plt.gca().set_title(subtitle)
        plt.imshow(circle, 'gray')

        # print("\n#", i+1)
        # plt.figure(figsize=(1,1))
        # plt.imshow(circle, 'gray')
        # title = "Question #" + str(i+1)
        # plt.title(title)
        # plt.show()
        # print(answer)

    # ensure at least one answer is actually circled by confirming that "darkest"'s luminance is >= 1 SD from others' luminance
    lighter_circles_luminance = circle_luminance.copy()
    lighter_circles_luminance.remove(min(lighter_circles_luminance))
    lighter_circles_average_luminance = sum(lighter_circles_luminance) / len(lighter_circles_luminance)
    circle_luminance_std_dev = np.std(np.array(lighter_circles_luminance))

    if (not (abs(lighter_circles_average_luminance - min(circle_luminance)) > 2*circle_luminance_std_dev)):
      answer = 'None'
    # if(debug):
    #   if (abs(lighter_circles_average_luminance - min(circle_luminance)) > circle_luminance_std_dev):
    #     print("answer is likely singular", (lighter_circles_average_luminance - min(circle_luminance)))
    #   else:
    #     print("may be multiple answers selected", (lighter_circles_average_luminance - min(circle_luminance)))

    answers.append(answer)

    if(debug):
      title = "Question #" + str(i+1) 
      # plt.title(title)
      print(title)
      plt.axis('off')
      plt.show()

  return answers;

def parseTest(test, debug = False):
  test_outline = getPageOutline(test)
  
  if(debug):
    plt.figure(figsize=(figure_dims))
    plt.imshow(cv2.drawContours(test.copy(), [test_outline], -1, (0, 255, 0), 3))
    plt.axis('off')
    plt.title("Detected Page")
    plt.show()

  fullTest = affineTransformPage(test_outline, test)

  #enforce letter paper aspect ratio to correct for any minor errors in perspective transformation
  #fullTest.shape[1]/fullTest.shape[0] = 0.772727273
  newWidth = int(fullTest.shape[0]*0.772727273) #width/height = aspectRatio
  fullTest= cv2.resize(fullTest, (newWidth, fullTest.shape[0]), interpolation = cv2.INTER_AREA)

  fullTest = correctPageOrientation(fullTest, debug)



  if(debug):
    plt.figure(figsize=(figure_dims))
    plt.imshow(fullTest)
    plt.axis('off')
    plt.title("Cropped & Perspective Transformed Page")
    plt.show()

  # determine answers selected in input image
  circles = getCircles(fullTest, .15, True)

  grouped_circles = groupCircles(circles) #num_answers4

  answers = determineAnswer(fullTest, grouped_circles,debug);
  print("answers",answers)
  return fullTest, answers, grouped_circles

def extractText(img_OG, debug= False): #this function uses pytesseract to read the text from the test
  img = img_OG.copy()

  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

  custom_config = r'--oem 3 --psm 11'
  # Page segmentation modes:
  #   0    Orientation and script detection (OSD) only.
  #   1    Automatic page segmentation with OSD.
  #   2    Automatic page segmentation, but no OSD, or OCR.
  #   3    Fully automatic page segmentation, but no OSD. (Default)
  #   4    Assume a single column of text of variable sizes.
  #   5    Assume a single uniform block of vertically aligned text.
  #   6    Assume a single uniform block of text.
  #   7    Treat the image as a single text line.
  #   8    Treat the image as a single word.
  #   9    Treat the image as a single word in a circle.
  #  10    Treat the image as a single character.
  #  11    Sparse text. Find as much text as possible in no particular order.
  #  12    Sparse text with OSD.
  #  13    Raw line. Treat the image as a single text line,
  #                         bypassing hacks that are Tesseract-specific.

  text = pytesseract.image_to_string(img, config=custom_config)
  d = pytesseract.image_to_data(img, output_type=Output.DICT)
  keys = list(d.keys())

  n_boxes = len(d['text'])

  if(debug):  
    for i in range(n_boxes):
        if int(d['conf'][i]) > 60 or True:
          #if re.match(points_pattern, d['text'][i]):
              (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])
              img = cv2.rectangle(img_OG, (x, y), (x + w, y + h), (0, 255, 0), 2)

    plt.figure(figsize=(figure_dims))
    plt.imshow(img)
    plt.title("Detected Text")
    plt.axis('off')
    plt.show()
    
  return text;

def getWeights(text): #this function takes the test's texts, and extracts question weights
  pattern = '\([0-9]+ Points\)'
  weights = re.findall(pattern, text) 
  pattern2 = '[0-9]+'
  for i in range(len(weights)):
    weights[i] = (int(re.findall(pattern2, weights[i])[0]))

  return weights

def superimposeText(img, overlay_text = "dookie", text_origin = (0, 0)):
  # denotes font type
  font = cv2.FONT_HERSHEY_SIMPLEX

  # coordinates of the bottom-left corner of the text string in the image
  # text_origin = (50, 50) 

  # scale factor that is multiplied by the font-specific base size.
  fontScale = 2

  # color in BGR
  color = (255, 0, 0)
  
  # Line thickness in px
  thickness = 2

  img = cv2.putText(img, overlay_text, text_origin, font, fontScale, color, thickness, cv2.LINE_AA)

  return img;

def ABCD2Index(abcd):
  # convert alphabet character to integer, where 'A' = 0
  return ord(abcd) - 65

def superimposeCircles(img, circles, student_answers, answer_key):
  a, b, _c = circles.shape

  correct_color = (0, 255, 0) #green
  incorrect_color = (255, 0, 0) #red
  
  for i in range(a):
      answer_key_index = ABCD2Index(answer_key[i])

      #the following if block may be confusing... If student's answer is 'None',
      # we set the index to equal the answer key index so only the correct answer is highlighted
      if(student_answers[i] != 'None'): 
        student_answer_index = ABCD2Index(student_answers[i])
      else:
        student_answer_index = answer_key_index  

      #change color of answer depending on if it was selected correctly
      if (student_answer_index == answer_key_index):
        cv2.circle(img, (circles[i][answer_key_index][0], circles[i][answer_key_index][1]), circles[i][answer_key_index][2], correct_color, 15, cv2.LINE_AA)
      else:
        cv2.circle(img, (circles[i][answer_key_index][0], circles[i][answer_key_index][1]), circles[i][answer_key_index][2], correct_color, 15, cv2.LINE_AA)
        cv2.circle(img, (circles[i][student_answer_index][0], circles[i][student_answer_index][1]), circles[i][student_answer_index][2], incorrect_color, 15, cv2.LINE_AA)

        # cv2.circle(img, (circles[i][j][0], circles[i][j][1]), circles[i][j][2], color, -1, cv2.LINE_AA)

  return img

cd = "/content/drive/MyDrive/image_data"

def interpretKey(testkey):
  # get the cropped testkey and marked answers 
  testkey_full, answer_key, dummy_var = parseTest(testkey, True) 
  print("Finished parsing testkey!")
  test_text = extractText(testkey_full) 
  # print("test_text", test_text)

  weights = getWeights(test_text)
  print("Finished extracting question weights: ", weights)
  possible_points = np.array(weights).sum()

  return weights, possible_points, answer_key



def gradeExam(weights, possible_points, answer_key, studenttest):
  exam_image_name = studenttest
  studenttest = io.imread(studenttest)

  studenttest_full, student_answers, answer_bubble_location = parseTest(studenttest, True)
  print("Finished parsing student's test!")

  # compares the student answers to the test key to create array. 1 for correct answers, 0 for incorrect answers
  student_answer_correctness = (np.array(answer_key) == np.array(student_answers))*1 

  #multiply previous array by the weights to get an array of which the sum is the students total grade
  student_grade_per_question = np.array(weights) * student_answer_correctness 
  student_total_grade = student_grade_per_question.sum()

  print("exam: ", exam_image_name)
  print(student_grade_per_question)
  print(student_total_grade, "/", possible_points)

  graded_test = superimposeText(studenttest_full.copy(), str(student_total_grade) + "/"+ str(possible_points), (int(studenttest_full.shape[1]*.72),(int(studenttest_full.shape[0]*.055))))
  circled_graded_test = superimposeCircles(graded_test, np.array(answer_bubble_location), student_answers, answer_key);
  plt.figure(figsize=(figure_dims))
  plt.imshow(circled_graded_test)
  plt.title("Graded Test")
  plt.axis('off')
  plt.show()

  # cv2.imwrite(str(cd + "/" + exam_image_name + "_graded.png") , circled_graded_test)
  # with open(str(cd + "/" + exam_image_name + "_graded.png"), 'w') as f:
  #   f.write(circled_graded_test)

  # return str(exam_image_name + ": " + str(student_total_grade) + "/" + str(possible_points))  
  return str(exam_image_name + ": " + str(student_answers))

"""# Batch Processing for Statistics"""

import os, re
cd = "/content/drive/MyDrive/image_data"

A_key = io.imread('/content/drive/MyDrive/image_data/A_key.jpg')
B_key = io.imread('/content/drive/MyDrive/image_data/B_key.jpg')
C_key = io.imread('/content/drive/MyDrive/image_data/C_key.jpg')

exam_keys = [A_key, B_key, C_key]


# print(os.listdir(cd))
# for file in os.listdir(cd):
#   print(re.match("[a-zA-Z]+_A+_\d\.jpg", file))

A_exams = [file for file in os.listdir(cd) if re.match("[a-zA-Z]+_A+_\d\.jpg", file)]
B_exams = [file for file in os.listdir(cd) if re.match("[a-zA-Z]+_B+_\d\.jpg", file)]
C_exams = [file for file in os.listdir(cd) if re.match("[a-zA-Z]+_C+_\d\.jpg", file)]

exams = [A_exams, B_exams, C_exams]



all_results = []
for i in range(3):
# for exam in exams:
  weights, possible_points, answer_key = interpretKey(exam_keys[i]) # A, B, C, ...

  results = []
  for j in range(len(exams[i])):
    try:
      results.append(gradeExam(weights, possible_points, answer_key, str(cd + "/" + exams[i][j])))
    except Exception as e:
      results.append(str(exams[i][j] + ": Exception occurred: " + str(e)))

  all_results.append(results)

print(all_results)

"""# `main`"""

#@title choose images: { form-width: "25%" }
studenttest = '/content/drive/MyDrive/image_data/mk_A_1.jpg' #@param {type:"string"}
testkey = '/content/drive/MyDrive/image_data/A_key.jpg' #@param {type:"string"}
studenttest = io.imread(studenttest) 
testkey = io.imread(testkey) 

# get the cropped testkey and marked answers 
testkey_full, answer_key, dummy_var = parseTest(testkey, False) 
print("Finished parsing answer key")

test_text = extractText(testkey_full, True) 
print("Finished parsing question text")#: ", test_text)

weights = getWeights(test_text)
possible_points = np.array(weights).sum()
print("Finished extracting question weights")

studenttest_full, student_answers, answer_bubble_location = parseTest(studenttest, True)
print("Finished parsing student test")

# compares the student answers to the test key to create array. 1 for correct answers, 0 for incorrect answers
student_answer_correctness = (np.array(answer_key) == np.array(student_answers))*1 

#multiply previous array by the weights to get an array of which the sum is the students total grade
student_grade_per_question = np.array(weights) * student_answer_correctness 
student_total_grade = student_grade_per_question.sum()
print("Finished grading test")
# print(student_grade_per_question)
# print(student_total_grade, "/",possible_points)

graded_test = superimposeText(studenttest_full.copy(), str(student_total_grade) + "/"+ str(possible_points), (int(studenttest_full.shape[1]*.72),(int(studenttest_full.shape[0]*.055))))
circled_graded_test = superimposeCircles(graded_test, np.array(answer_bubble_location), student_answers, answer_key);
print("Finished superimposing grade on test")

plt.figure(figsize=(figure_dims))
plt.imshow(circled_graded_test)
plt.title("Graded Exam")
plt.axis('off')
plt.show()